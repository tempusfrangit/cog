{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cog: Containers for machine learning","text":"<p>Cog is an open-source tool that lets you package machine learning models in a standard, production-ready container.</p> <p>You can deploy your packaged model to your own infrastructure, or to Replicate.</p>"},{"location":"#highlights","title":"Highlights","text":"<ul> <li> <p>\ud83d\udce6 Docker containers without the pain. Writing your own <code>Dockerfile</code> can be a bewildering process. With Cog, you define your environment with a simple configuration file and it generates a Docker image with all the best practices: Nvidia base images, efficient caching of dependencies, installing specific Python versions, sensible environment variable defaults, and so on.</p> </li> <li> <p>\ud83e\udd2c\ufe0f No more CUDA hell. Cog knows which CUDA/cuDNN/PyTorch/Tensorflow/Python combos are compatible and will set it all up correctly for you.</p> </li> <li> <p>\u2705 Define the inputs and outputs for your model with standard Python. Then, Cog generates an OpenAPI schema and validates the inputs and outputs with Pydantic.</p> </li> <li> <p>\ud83c\udf81 Automatic HTTP prediction server: Your model's types are used to dynamically generate a RESTful HTTP API using FastAPI.</p> </li> <li> <p>\ud83e\udd5e Automatic queue worker. Long-running deep learning models or batch processing is best architected with a queue. Cog models do this out of the box. Redis is currently supported, with more in the pipeline.</p> </li> <li> <p>\u2601\ufe0f Cloud storage. Files can be read and written directly to Amazon S3 and Google Cloud Storage. (Coming soon.)</p> </li> <li> <p>\ud83d\ude80 Ready for production. Deploy your model anywhere that Docker images run. Your own infrastructure, or Replicate.</p> </li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<p>Define the Docker environment your model runs in with <code>cog.yaml</code>:</p> <pre><code>build:\n  gpu: true\n  system_packages:\n    - \"libgl1-mesa-glx\"\n    - \"libglib2.0-0\"\n  python_version: \"3.11\"\n  python_packages:\n    - \"torch==1.8.1\"\npredict: \"predict.py:Predictor\"\n</code></pre> <p>Define how predictions are run on your model with <code>predict.py</code>:</p> <pre><code>from cog import BasePredictor, Input, Path\nimport torch\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        self.model = torch.load(\"./weights.pth\")\n\n    # The arguments and types the model takes as input\n    def predict(self,\n          image: Path = Input(description=\"Grayscale input image\")\n    ) -&gt; Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        processed_image = preprocess(image)\n        output = self.model(processed_image)\n        return postprocess(output)\n</code></pre> <p>Now, you can run predictions on this model:</p> <pre><code>$ cog predict -i image=@input.jpg\n--&gt; Building Docker image...\n--&gt; Running Prediction...\n--&gt; Output written to output.jpg\n</code></pre> <p>Or, build a Docker image for deployment:</p> <pre><code>$ cog build -t my-colorization-model\n--&gt; Building Docker image...\n--&gt; Built my-colorization-model:latest\n\n$ docker run -d -p 5000:5000 --gpus all my-colorization-model\n\n$ curl http://localhost:5000/predictions -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"input\": {\"image\": \"https://.../input.jpg\"}}'\n</code></pre>"},{"location":"#why-are-we-building-this","title":"Why are we building this?","text":"<p>It's really hard for researchers to ship machine learning models to production.</p> <p>Part of the solution is Docker, but it is so complex to get it to work: Dockerfiles, pre-/post-processing, Flask servers, CUDA versions. More often than not the researcher has to sit down with an engineer to get the damn thing deployed.</p> <p>Andreas and Ben created Cog. Andreas used to work at Spotify, where he built tools for building and deploying ML models with Docker. Ben worked at Docker, where he created Docker Compose.</p> <p>We realized that, in addition to Spotify, other companies were also using Docker to build and deploy machine learning models. Uber and others have built similar systems. So, we're making an open source version so other people can do this too.</p> <p>Hit us up if you're interested in using it or want to collaborate with us. We're on Discord or email us at team@replicate.com.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS, Linux or Windows 11. Cog works on macOS, Linux and Windows 11 with WSL 2</li> <li>Docker. Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog. If you install Docker Engine instead of Docker Desktop, you will need to install Buildx as well.</li> </ul>"},{"location":"#install","title":"Install","text":"<p>If you're using macOS, you can install Cog using Homebrew:</p> <pre><code>brew install cog\n</code></pre> <p>You can also download and install the latest release of Cog directly from GitHub by running the following commands in a terminal:</p> <pre><code>sudo curl -o /usr/local/bin/cog -L \"https://github.com/replicate/cog/releases/latest/download/cog_$(uname -s)_$(uname -m)\"\nsudo chmod +x /usr/local/bin/cog\n</code></pre> <p>Alternatively, you can build Cog from source and install it with these commands:</p> <pre><code>make\nsudo make install\n</code></pre>"},{"location":"#upgrade","title":"Upgrade","text":"<p>If you previously installed Cog from a GitHub Releases URL, you can upgrade to the latest version by running the same commands you used to install it:</p> <pre><code>sudo curl -o /usr/local/bin/cog -L \"https://github.com/replicate/cog/releases/latest/download/cog_$(uname -s)_$(uname -m)\"\nsudo chmod +x /usr/local/bin/cog\n</code></pre> <p>If you're using macOS and you previously installed Cog with Homebrew, run the following:</p> <pre><code>brew upgrade cog\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Get started with an example model</li> <li>Get started with your own model</li> <li>Using Cog with notebooks</li> <li>Using Cog with Windows 11</li> <li>Take a look at some examples of using Cog</li> <li>Deploy models with Cog</li> <li><code>cog.yaml</code> reference to learn how to define your model's environment</li> <li>Prediction interface reference to learn how the <code>Predictor</code> interface works</li> <li>Training interface reference to learn how to add a fine-tuning API to your model</li> <li>HTTP API reference to learn how to use the HTTP API that models serve</li> </ul>"},{"location":"#need-help","title":"Need help?","text":"<p>Join us in #cog on Discord.</p>"},{"location":"#contributors","title":"Contributors \u2728","text":"<p>Thanks goes to these wonderful people (emoji key):</p> <sub>Ben Firshman</sub>\ud83d\udcbb \ud83d\udcd6 <sub>Andreas Jansson</sub>\ud83d\udcbb \ud83d\udcd6 \ud83d\udea7 <sub>Zeke Sikelianos</sub>\ud83d\udcbb \ud83d\udcd6 \ud83d\udd27 <sub>Rory Byrne</sub>\ud83d\udcbb \ud83d\udcd6 \u26a0\ufe0f <sub>Michael Floering</sub>\ud83d\udcbb \ud83d\udcd6 \ud83e\udd14 <sub>Ben Evans</sub>\ud83d\udcd6 <sub>shashank agarwal</sub>\ud83d\udcbb \ud83d\udcd6 <sub>VictorXLR</sub>\ud83d\udcbb \ud83d\udcd6 \u26a0\ufe0f <sub>hung anna</sub>\ud83d\udc1b <sub>Brian Whitman</sub>\ud83d\udc1b <sub>JimothyJohn</sub>\ud83d\udc1b <sub>ericguizzo</sub>\ud83d\udc1b <sub>Dominic Baggott</sub>\ud83d\udcbb \u26a0\ufe0f <sub>Dashiell Stander</sub>\ud83d\udc1b \ud83d\udcbb \u26a0\ufe0f <sub>Shuwei Liang</sub>\ud83d\udc1b \ud83d\udcac <sub>Eric Allam</sub>\ud83e\udd14 <sub>Iv\u00e1n Perdomo</sub>\ud83d\udc1b <sub>Charles Frye</sub>\ud83d\udcd6 <sub>Luan Pham</sub>\ud83d\udc1b \ud83d\udcd6 <sub>TommyDew</sub>\ud83d\udcbb <sub>Jesse Andrews</sub>\ud83d\udcbb \ud83d\udcd6 \u26a0\ufe0f <sub>Nick Stenning</sub>\ud83d\udcbb \ud83d\udcd6 \ud83c\udfa8 \ud83d\ude87 \u26a0\ufe0f <sub>Justin Merrell</sub>\ud83d\udcd6 <sub>Rurik Yl\u00e4-Onnenvuori</sub>\ud83d\udc1b <sub>Youka</sub>\ud83d\udc1b <sub>Clay Mullis</sub>\ud83d\udcd6 <sub>Mattt</sub>\ud83d\udcbb \ud83d\udcd6 \ud83d\ude87 <sub>Eng Zer Jun</sub>\u26a0\ufe0f <sub>BB</sub>\ud83d\udcbb <sub>williamluer</sub>\ud83d\udcd6 <sub>Simon Eskildsen</sub>\ud83d\udcbb <sub>F</sub>\ud83d\udc1b \ud83d\udcbb <sub>Philip Potter</sub>\ud83d\udc1b \ud83d\udcbb <sub>Joanne Chen</sub>\ud83d\udcd6 <sub>technillogue</sub>\ud83d\udcbb <sub>Aron Carroll</sub>\ud83d\udcd6 \ud83d\udcbb \ud83e\udd14 <p>This project follows the all-contributors specification. Contributions of any kind welcome!</p>"},{"location":"CONTRIBUTING/","title":"Contributing guide","text":""},{"location":"CONTRIBUTING/#making-a-contribution","title":"Making a contribution","text":""},{"location":"CONTRIBUTING/#signing-your-work","title":"Signing your work","text":"<p>Each commit you contribute to Cog must be signed off (not to be confused with signing). It certifies that you wrote the patch, or have the right to contribute it. It is called the Developer Certificate of Origin and was originally developed for the Linux kernel.</p> <p>If you can certify the following:</p> <pre><code>By making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n</code></pre> <p>Then add this line to each of your Git commit messages, with your name and email:</p> <pre><code>Signed-off-by: Sam Smith &lt;sam.smith@example.com&gt;\n</code></pre>"},{"location":"CONTRIBUTING/#how-to-sign-off-your-commits","title":"How to sign off your commits","text":"<p>If you're using the <code>git</code> CLI, you can sign a commit by passing the <code>-s</code> option: <code>git commit -s -m \"Reticulate splines\"</code></p> <p>You can also create a git hook which will sign off all your commits automatically. Using hooks also allows you to sign off commits when using non-command-line tools like GitHub Desktop or VS Code.</p> <p>First, create the hook file and make it executable:</p> <pre><code>cd your/checkout/of/cog\ntouch .git/hooks/prepare-commit-msg\nchmod +x .git/hooks/prepare-commit-msg\n</code></pre> <p>Then paste the following into the file:</p> <pre><code>#!/bin/sh\n\nNAME=$(git config user.name)\nEMAIL=$(git config user.email)\n\nif [ -z \"$NAME\" ]; then\n    echo \"empty git config user.name\"\n    exit 1\nfi\n\nif [ -z \"$EMAIL\" ]; then\n    echo \"empty git config user.email\"\n    exit 1\nfi\n\ngit interpret-trailers --if-exists doNothing --trailer \\\n    \"Signed-off-by: $NAME &lt;$EMAIL&gt;\" \\\n    --in-place \"$1\"\n</code></pre>"},{"location":"CONTRIBUTING/#acknowledging-contributions","title":"Acknowledging contributions","text":"<p>We welcome contributions from everyone, and consider all forms of contribution equally valuable. This includes code, bug reports, feature requests, and documentation. We use All Contributors to maintain a list of all the people who have contributed to Cog.</p> <p>To acknowledge a contribution, add a comment to an issue or pull request in the following format:</p> <pre><code>@allcontributors please add @username for doc,code,ideas\n</code></pre> <p>A bot will automatically open a pull requests to add the contributor to the project README.</p> <p>Common contribution types include: <code>doc</code>, <code>code</code>, <code>bug</code>, and <code>ideas</code>. See the full list at allcontributors.org/docs/en/emoji-key</p>"},{"location":"CONTRIBUTING/#development-environment","title":"Development environment","text":"<p>You'll need to install Go 1.21. If you're using a newer Mac with an M1 chip, be sure to download the <code>darwin-arm64</code> installer package. Alternatively you can run <code>brew install go</code> which will automatically detect and use the appropriate installer for your system architecture.</p> <p>Install the Python dependencies:</p> <pre><code>python -m pip install '.[dev]'\n</code></pre> <p>Once you have Go installed, run:</p> <pre><code>make install PREFIX=$(go env GOPATH)\n</code></pre> <p>This installs the <code>cog</code> binary to <code>$GOPATH/bin/cog</code>.</p> <p>To run the tests:</p> <pre><code>make test\n</code></pre> <p>The project is formatted by goimports. To format the source code, run:</p> <pre><code>make fmt\n</code></pre> <p>If you encounter any errors, see the troubleshooting section below?</p>"},{"location":"CONTRIBUTING/#project-structure","title":"Project structure","text":"<p>As much as possible, this is attempting to follow the Standard Go Project Layout.</p> <ul> <li><code>cmd/</code> - The root <code>cog</code> command.</li> <li><code>pkg/cli/</code> - CLI commands.</li> <li><code>pkg/config</code> - Everything <code>cog.yaml</code> related.</li> <li><code>pkg/docker/</code> - Low-level interface for Docker commands.</li> <li><code>pkg/dockerfile/</code> - Creates Dockerfiles.</li> <li><code>pkg/image/</code> - Creates and manipulates Cog Docker images.</li> <li><code>pkg/predict/</code> - Runs predictions on models.</li> <li><code>pkg/util/</code> - Various packages that aren't part of Cog. They could reasonably be separate re-usable projects.</li> <li><code>python/</code> - The Cog Python library.</li> <li><code>test-integration/</code> - High-level integration tests for Cog.</li> </ul>"},{"location":"CONTRIBUTING/#concepts","title":"Concepts","text":"<p>There are a few concepts used throughout Cog that might be helpful to understand.</p> <ul> <li>Config: The <code>cog.yaml</code> file.</li> <li>Image: Represents a built Docker image that serves the Cog API, containing a model.</li> <li>Input: Input from a prediction, as key/value JSON object.</li> <li>Model: A user's machine learning model, consisting of code and weights.</li> <li>Output: Output from a prediction, as arbitrarily complex JSON object.</li> <li>Prediction: A single run of the model, that takes input and produces output.</li> <li>Predictor: Defines how Cog runs predictions on a model.</li> </ul>"},{"location":"CONTRIBUTING/#running-tests","title":"Running tests","text":"<p>To run the entire test suite:</p> <pre><code>make test\n</code></pre> <p>To run just the Golang tests:</p> <pre><code>make test-go\n</code></pre> <p>To run just the Python tests:</p> <pre><code>make test-python\n</code></pre> <p>To stand up a server for one of the integration tests:</p> <pre><code>make install\npip install -r requirements-dev.txt\nmake test\ncd test-integration/test_integration/fixtures/file-project\ncog build\ndocker run -p 5001:5000 --init --platform=linux/amd64 cog-file-project\n</code></pre> <p>Then visit localhost:5001 in your browser.</p>"},{"location":"CONTRIBUTING/#running-the-docs-server","title":"Running the docs server","text":"<p>To run the docs website server locally:</p> <pre><code>make run-docs-server\n</code></pre>"},{"location":"CONTRIBUTING/#publishing-a-release","title":"Publishing a release","text":"<p>This project has a GitHub Actions workflow that uses goreleaser to facilitate the process of publishing new releases. The release process is triggered by manually creating and pushing a new git tag.</p> <p>To publish a new release, run the following in your local checkout of cog:</p> <pre><code>git checkout main\ngit fetch --all --tags\ngit tag v0.0.11\ngit push --tags\n</code></pre> <p>Then visit github.com/replicate/cog/actions to monitor the release process.</p>"},{"location":"CONTRIBUTING/#publishing-a-prerelease","title":"Publishing a prerelease","text":"<p>Prereleases are a useful way to give testers a way to try out new versions of Cog without affecting the documented <code>latest</code> download URL which people normally use to install Cog.</p> <p>To publish a prerelease version, append a SemVer prerelease identifer like <code>-alpha</code> or <code>-beta</code> to the git tag name. Goreleaser will detect this and mark it as a prerelease in GitHub Releases.</p> <pre><code>git checkout some-prerelease-branch\ngit fetch --all --tags\ngit tag -a v0.1.0-alpha -m \"Prerelease v0.1.0\"\ngit push --tags\n</code></pre>"},{"location":"CONTRIBUTING/#troubleshooting","title":"Troubleshooting","text":""},{"location":"CONTRIBUTING/#cog-command-not-found","title":"<code>cog command not found</code>","text":"<p>The compiled <code>cog</code> binary will be installed in <code>$GOPATH/bin/cog</code>, e.g. <code>~/go/bin/cog</code>. Make sure that Golang's bin directory is present on your system PATH by adding it to your shell config (<code>.bashrc</code>, <code>.zshrc</code>, etc):</p> <pre><code>export PATH=~/go/bin:$PATH\n</code></pre> <p>Still having trouble? Please open an issue on GitHub.</p>"},{"location":"deploy/","title":"Deploy models with Cog","text":"<p>Cog containers are Docker containers that serve an HTTP server for running predictions on your model. You can deploy them anywhere that Docker containers run.</p> <p>This guide assumes you have a model packaged with Cog. If you don't, follow our getting started guide, or use an example model.</p>"},{"location":"deploy/#getting-started","title":"Getting started","text":"<p>First, build your model:</p> <pre><code>cog build -t my-model\n</code></pre> <p>Then, start the Docker container:</p> <pre><code>docker run -d -p 5000:5000 my-model\n\n# If your model uses a GPU:\ndocker run -d -p 5000:5000 --gpus all my-model\n\n# If you're on an M1 Mac:\ndocker run -d -p 5000:5000 --platform=linux/amd64 my-model\n</code></pre> <p>Port 5000 is now serving the API:</p> <pre><code>curl http://localhost:5000\n</code></pre> <p>To run a prediction on the model, call the <code>/predictions</code> endpoint, passing input in the format expected by your model:</p> <pre><code>curl http://localhost:5000/predictions -X POST \\\n    --data '{\"input\": {\"image\": \"https://.../input.jpg\"}}'\n</code></pre> <p>To view the API documentation in browser for the model that is running, open http://localhost:5000/docs.</p> <p>For more details about the HTTP API, see the HTTP API reference documentation.</p>"},{"location":"deploy/#options","title":"Options","text":"<p>Cog Docker images have <code>python -m cog.server.http</code> set as the default command, which gets overridden if you pass a command to <code>docker run</code>. When you use command-line options, you need to pass in the full command before the options.</p>"},{"location":"deploy/#-threads","title":"<code>--threads</code>","text":"<p>This controls how many threads are used by Cog, which determines how many requests Cog serves in parallel. If your model uses a CPU, this is the number of CPUs on your machine. If your model uses a GPU, this is 1, because typically a GPU can only be used by one process.</p> <p>You might need to adjust this if you want to control how much memory your model uses, or other similar constraints. To do this, you can use the <code>--threads</code> option.</p> <p>For example:</p> <pre><code>docker run -d -p 5000:5000 my-model python -m cog.server.http --threads=10\n</code></pre>"},{"location":"environment/","title":"Environment Variables","text":"<p>This guide provides a list of environment variables that can be set that impact the execution of Cog. This assumes that you have already followed the main steps to set up Cog and have a general familiarity with how to run Cog.</p>"},{"location":"environment/#general","title":"General","text":"<p>This section lists the relevant environment variables for general execution of Cog.</p>"},{"location":"environment/#cgo_enabled","title":"<code>CGO_ENABLED</code>","text":"<p>This variable determines whether the usage of cgo should be enabled or disabled. cgo is a subsystem in Go that enables the programmer to invoke C code from Go packages.</p> <p>This can be set to either 0 or 1 to enable/disable cgo. By default, it is set to 0 in order to create statically linked binaries that can help with the portability of containers by ensuring that the binary is not reliant on shared libraries provided with a source image.</p>"},{"location":"environment/#cog_no_update_check","title":"<code>COG_NO_UPDATE_CHECK</code>","text":"<p>This determines whether there should be an update check or not. An update check will display an update message if an update is available and will check for a new update in the background. The result of that check will then be displayed the next time the user runs Cog.</p> <p>This can be either set/unset in order to disable/enable the update checks. By default, it is not set.</p>"},{"location":"environment/#log_format","title":"<code>LOG_FORMAT</code>","text":"<p>This determines what format to output the logs. Specifically, if set to \"development\", then it will switch to a human-friendly log output.</p> <p>This can be set to development or entirely omitted. By default, the log format will be left unset and thus not have the human-friendly output to the logs.</p>"},{"location":"environment/#model","title":"Model","text":"<p>This section lists the relevant environment variables for models.</p>"},{"location":"environment/#cog_weights","title":"<code>COG_WEIGHTS</code>","text":"<p>This specifies the URLs to the paths or files for any model weights that will be used to prepare the model.</p> <p>This can be set to a string that specifies the path/file. By default, it is not set to anything.</p>"},{"location":"environment/#hostname","title":"<code>HOSTNAME</code>","text":"<p>This specifies the hostname for the model in the span attributes.</p> <p>This can be set to a string that specifies the hostname for the model. By default, it is not set to anything.</p>"},{"location":"environment/#cog_model_id","title":"<code>COG_MODEL_ID</code>","text":"<p>This specifies the model id for the model in the span attributes. Model id is a unique ID for the Cog model that is used to label setup logs (this is subject to change soon with logging changes in 0.3.0).</p> <p>This can be set to a string that specifies the model id for the model. By default, it is not set to anything.</p>"},{"location":"environment/#cog_model_name","title":"<code>COG_MODEL_NAME</code>","text":"<p>This specifies the model name for the model in the span attributes.</p> <p>This can be set to a string that specifies the model name for the model. By default, it is not set to anything.</p>"},{"location":"environment/#cog_username","title":"<code>COG_USERNAME</code>","text":"<p>This specifies the username for the model in the span attributes.</p> <p>This can be set to a string that specifies the username for the model. By default, it is not set to anything.</p>"},{"location":"environment/#cog_model_version","title":"<code>COG_MODEL_VERSION</code>","text":"<p>This specifies the version for the model in the span attributes.</p> <p>This can be set to a string that specifies the version for the model. By default, the program will assume the value of the prediction's version to be the same for the model version if there is a prediction, otherwise it will be set to nothing.</p>"},{"location":"environment/#cog_hardware","title":"<code>COG_HARDWARE</code>","text":"<p>This specifies the hardware for the model in the span attributes.</p> <p>This can be set to a string that specifies the hardware for the model. By default, it is not set to anything.</p>"},{"location":"environment/#cog_docker_image_uri","title":"<code>COG_DOCKER_IMAGE_URI</code>","text":"<p>This specifies the docker image URI for the model in the span attributes.</p> <p>This can be set to a string that specifies the docker image URI for the model. By default, it is not set to anything.</p>"},{"location":"environment/#server","title":"Server","text":"<p>This section lists the relevant environment variables for running the HTTP API when making predictions.</p>"},{"location":"environment/#cog_log_level","title":"<code>COG_LOG_LEVEL</code>","text":"<p>This defines what type of messages are reported/displayed when running the HTTP server that makes the cog predictions.</p> <p>It can be set to info, debug, or warning. By default, the info log level is used if not supplied.</p>"},{"location":"environment/#port","title":"<code>PORT</code>","text":"<p>This defines what port is exposed from the container for the HTTP server to be hosted on.</p> <p>This can be set to any valid port number. By default, the port number will be set to 5000.</p>"},{"location":"environment/#cog_throttle_response_interval","title":"<code>COG_THROTTLE_RESPONSE_INTERVAL</code>","text":"<p>This specifies the duration that the server should wait before sending another response, as handled by the ResponseThrottler.</p> <p>This can be set to a float, which will represent the number of seconds to wait between responses. By default, this will be set to 0.5.</p>"},{"location":"environment/#webhook_auth_token","title":"<code>WEBHOOK_AUTH_TOKEN</code>","text":"<p>This specifies the authentication token (if necessary) in order to be authorized for a webhook call.</p> <p>This can be set to the string representing your authentication token. By default, this will be set to nothing.</p>"},{"location":"environment/#kubernetes_service_host","title":"<code>KUBERNETES_SERVICE_HOST</code>","text":"<p>This determines whether or not to run Cog with Kubernetes. Running with Kubernetes will result in Cog setting up probe helpers, which is what kubelets use in Kuberenetes to determine when to restart containers, when containers are ready for traffic, and when container applications have started.</p> <p>This can be set / unset. By default, it is unset.</p>"},{"location":"environment/#redis","title":"Redis","text":"<p>This section lists the relevant environment variables for Cog's built-in queue worker to process predictions from a Redis queue.</p>"},{"location":"environment/#otel_service_name","title":"<code>OTEL_SERVICE_NAME</code>","text":"<p>This determines whether to enable or disable the usage of OpenTelemetry. OpenTelemetry (OTEL) is an open-source technology used to capture and measure metrics, traces, and logs. It is used for Cog's queue worker.</p> <p>This can either be set / unset in order to determine whether or not to enable OpenTelemtry. If it is set, then Cog will handle the necessary setup for OpenTelemtry. Otherwise, OpenTelemetry calls will be treated as no-ops. If OpenTelemetry is enabled, the OTLP exporter may also need to be configured via environment variables.</p>"},{"location":"environment/#docker-image","title":"Docker Image","text":""},{"location":"environment/#nvidia_driver_capabilities","title":"<code>NVIDIA_DRIVER_CAPABILITIES</code>","text":"<p>This controls which Nvidia driver libraries/binaries will be mounted inside the container. The generated Docker image will set this to  <code>all</code> which will mount all Nvidia driver libraries/binaries inside the container beyond the default <code>utility</code> and <code>compute</code> capabilities.</p> <p><code>graphics</code>, <code>video</code>, and <code>display</code> add additional interesting capabilities beyond the default that may be useful for some models running in the container. <code>graphics</code> is required for accelerated OpenGL support. <code>video</code> is required for accelerated video encoding/decoding. <code>display</code> is required for accelerated X11 support.</p> <p>This is set to <code>all</code>, is non-configurable, and is documented here as an environment variable of interest. Setting or changing this during runtime inside the image will have no effect.</p>"},{"location":"getting-started-own-model/","title":"Getting started with your own model","text":"<p>This guide will show you how to put your own machine learning model in a Docker image using Cog. If you haven't got a model to try out, you'll want to follow the main getting started guide.</p>"},{"location":"getting-started-own-model/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS or Linux. Cog works on macOS and Linux, but does not currently support Windows.</li> <li>Docker. Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog.</li> </ul>"},{"location":"getting-started-own-model/#initialization","title":"Initialization","text":"<p>First, install Cog if you haven't already:</p> <pre><code>sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`\nsudo chmod +x /usr/local/bin/cog\n</code></pre> <p>To configure your project for use with Cog, you'll need to add two files:</p> <ul> <li><code>cog.yaml</code> defines system requirements, Python package dependencies, etc</li> <li><code>predict.py</code> describes the prediction interface for your model</li> </ul> <p>Use the <code>cog init</code> command to generate these files in your project:</p> <pre><code>$ cd path/to/your/model\n$ cog init\n</code></pre>"},{"location":"getting-started-own-model/#define-the-docker-environment","title":"Define the Docker environment","text":"<p>The <code>cog.yaml</code> file defines all the different things that need to be installed for your model to run. You can think of it as a simple way of defining a Docker image.</p> <p>For example:</p> <pre><code>build:\n  python_version: \"3.11\"\n  python_packages:\n    - \"torch==2.0.1\"\n</code></pre> <p>This will generate a Docker image with Python 3.11 and PyTorch 2 installed, for both CPU and GPU, with the correct version of CUDA, and various other sensible best-practices.</p> <p>To run a command inside this environment, prefix it with <code>cog run</code>:</p> <pre><code>$ cog run python\n\u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981\nRunning 'python' in Docker with the current directory mounted as a volume...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nPython 3.11.1 (main, Jan 27 2023, 10:52:46)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>This is handy for ensuring a consistent environment for development or training.</p> <p>With <code>cog.yaml</code>, you can also install system packages and other things. Take a look at the full reference to see what else you can do.</p>"},{"location":"getting-started-own-model/#define-how-to-run-predictions","title":"Define how to run predictions","text":"<p>The next step is to update <code>predict.py</code> to define the interface for running predictions on your model. The <code>predict.py</code> generated by <code>cog init</code> looks something like this:</p> <pre><code>from cog import BasePredictor, Path, Input\nimport torch\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        self.net = torch.load(\"weights.pth\")\n\n    def predict(self,\n            image: Path = Input(description=\"Image to enlarge\"),\n            scale: float = Input(description=\"Factor to scale image by\", default=1.5)\n    ) -&gt; Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        # ... pre-processing ...\n        output = self.net(input)\n        # ... post-processing ...\n        return output\n</code></pre> <p>Edit your <code>predict.py</code> file and fill in the functions with your own model's setup and prediction code. You might need to import parts of your model from another file.</p> <p>You also need to define the inputs to your model as arguments to the <code>predict()</code> function, as demonstrated above. For each argument, you need to annotate with a type. The supported types are:</p> <ul> <li><code>str</code>: a string</li> <li><code>int</code>: an integer</li> <li><code>float</code>: a floating point number</li> <li><code>bool</code>: a boolean</li> <li><code>cog.File</code>: a file-like object representing a file</li> <li><code>cog.Path</code>: a path to a file on disk</li> </ul> <p>You can provide more information about the input with the <code>Input()</code> function, as shown above. It takes these basic arguments:</p> <ul> <li><code>description</code>: A description of what to pass to this input for users of the model</li> <li><code>default</code>: A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to <code>None</code>, the input is optional.</li> <li><code>ge</code>: For <code>int</code> or <code>float</code> types, the value should be greater than or equal to this number.</li> <li><code>le</code>: For <code>int</code> or <code>float</code> types, the value should be less than or equal to this number.</li> <li><code>choices</code>: For <code>str</code> or <code>int</code> types, a list of possible values for this input.</li> </ul> <p>There are some more advanced options you can pass, too. For more details, take a look at the prediction interface documentation.</p> <p>Next, add the line <code>predict: \"predict.py:Predictor\"</code> to your <code>cog.yaml</code>, so it looks something like this:</p> <pre><code>build:\n  python_version: \"3.11\"\n  python_packages:\n    - \"torch==2.0.1\"\npredict: \"predict.py:Predictor\"\n</code></pre> <p>That's it! To test this works, try running a prediction on the model:</p> <pre><code>$ cog predict -i image=@input.jpg\n\u2713 Building Docker image from cog.yaml... Successfully built 664ef88bc1f4\n\u2713 Model running in Docker image 664ef88bc1f4\n\nWritten output to output.png\n</code></pre> <p>To pass more inputs to the model, you can add more <code>-i</code> options:</p> <pre><code>$ cog predict -i image=@image.jpg -i scale=2.0\n</code></pre> <p>In this case it is just a number, not a file, so you don't need the <code>@</code> prefix.</p>"},{"location":"getting-started-own-model/#using-gpus","title":"Using GPUs","text":"<p>To use GPUs with Cog, add the <code>gpu: true</code> option to the <code>build</code> section of your <code>cog.yaml</code>:</p> <pre><code>build:\n  gpu: true\n  ...\n</code></pre> <p>Cog will use the nvidia-docker base image and automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using.</p> <p>For more details, see the <code>gpu</code> section of the <code>cog.yaml</code> reference.</p>"},{"location":"getting-started-own-model/#next-steps","title":"Next steps","text":"<p>Next, you might want to take a look at:</p> <ul> <li>A guide explaining how to deploy a model.</li> <li>The reference for <code>cog.yaml</code></li> <li>The reference for the Python library</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>This guide will walk you through what you can do with Cog by using an example model.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>macOS or Linux. Cog works on macOS and Linux, but does not currently support Windows.</li> <li>Docker. Cog uses Docker to create a container for your model. You'll need to install Docker before you can run Cog.</li> </ul>"},{"location":"getting-started/#install-cog","title":"Install Cog","text":"<p>First, install Cog:</p> <pre><code>sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`\nsudo chmod +x /usr/local/bin/cog\n</code></pre>"},{"location":"getting-started/#create-a-project","title":"Create a project","text":"<p>Let's make a directory to work in:</p> <pre><code>mkdir cog-quickstart\ncd cog-quickstart\n</code></pre>"},{"location":"getting-started/#run-commands","title":"Run commands","text":"<p>The simplest thing you can do with Cog is run a command inside a Docker environment.</p> <p>The first thing you need to do is create a file called <code>cog.yaml</code>:</p> <pre><code>build:\n  python_version: \"3.11\"\n</code></pre> <p>Then, you can run any command inside this environment. For example, enter</p> <pre><code>cog run python\n</code></pre> <p>and you'll get an interactive Python shell:</p> <pre><code>\u2713 Building Docker image from cog.yaml... Successfully built 8f54020c8981\nRunning 'python' in Docker with the current directory mounted as a volume...\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nPython 3.11.1 (main, Jan 27 2023, 10:52:46)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <p>(Hit Ctrl-D to exit the Python shell.)</p> <p>Inside this Docker environment you can do anything \u2013\u00a0run a Jupyter notebook, your training script, your evaluation script, and so on.</p>"},{"location":"getting-started/#run-predictions-on-a-model","title":"Run predictions on a model","text":"<p>Let's pretend we've trained a model. With Cog, we can define how to run predictions on it in a standard way, so other people can easily run predictions on it without having to hunt around for a prediction script.</p> <p>First, run this to get some pre-trained model weights:</p> <pre><code>WEIGHTS_URL=https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\ncurl -O $WEIGHTS_URL\n</code></pre> <p>Then, we need to write some code to describe how predictions are run on the model.</p> <p>Save this to <code>predict.py</code>:</p> <pre><code>from typing import Any\nfrom cog import BasePredictor, Input, Path\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image as keras_image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        self.model = ResNet50(weights='resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n\n    # Define the arguments and types the model takes as input\n    def predict(self, image: Path = Input(description=\"Image to classify\")) -&gt; Any:\n        \"\"\"Run a single prediction on the model\"\"\"\n        # Preprocess the image\n        img = keras_image.load_img(image, target_size=(224, 224))\n        x = keras_image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        # Run the prediction\n        preds = self.model.predict(x)\n        # Return the top 3 predictions\n        return decode_predictions(preds, top=3)[0]\n</code></pre> <p>We also need to point Cog at this, and tell it what Python dependencies to install. Update <code>cog.yaml</code> to look like this:</p> <pre><code>build:\n  python_version: \"3.11\"\n  python_packages:\n    - pillow==9.5.0\n    - tensorflow==2.12.0\npredict: \"predict.py:Predictor\"\n</code></pre> <p>Let's grab an image to test the model with:</p> <pre><code>IMAGE_URL=https://gist.githubusercontent.com/bfirsh/3c2115692682ae260932a67d93fd94a8/raw/56b19f53f7643bb6c0b822c410c366c3a6244de2/mystery.jpg\ncurl $IMAGE_URL &gt; input.jpg\n</code></pre> <p>Now, let's run the model using Cog:</p> <pre><code>cog predict -i image=@input.jpg\n</code></pre> <p>If you see the following output</p> <pre><code>[\n  [\n    \"n02123159\",\n    \"tiger_cat\",\n    0.4874822497367859\n  ],\n  [\n    \"n02123045\",\n    \"tabby\",\n    0.23169134557247162\n  ],\n  [\n    \"n02124075\",\n    \"Egyptian_cat\",\n    0.09728282690048218\n  ]\n]\n</code></pre> <p>then it worked!</p> <p>Note: The first time you run <code>cog predict</code>, the build process will be triggered to generate a Docker container that can run your model. The next time you run <code>cog predict</code> the pre-built container will be used.</p>"},{"location":"getting-started/#build-an-image","title":"Build an image","text":"<p>We can bake your model's code, the trained weights, and the Docker environment into a Docker image. This image serves predictions with an HTTP server, and can be deployed to anywhere that Docker runs to serve real-time predictions.</p> <pre><code>cog build -t resnet\n# Building Docker image...\n# Built resnet:latest\n</code></pre> <p>Once you've built the image, you can optionally view the generated dockerfile to get a sense of what Cog is doing under the hood:</p> <pre><code>cog debug\n</code></pre> <p>You can run this image with <code>cog predict</code> by passing the filename as an argument:</p> <pre><code>cog predict resnet -i image=@input.jpg\n</code></pre> <p>Or, you can run it with Docker directly, and it'll serve an HTTP server:</p> <pre><code>docker run -d --rm -p 5000:5000 resnet\n</code></pre> <p>We can send inputs directly with <code>curl</code>:</p> <pre><code>curl http://localhost:5000/predictions -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"input\": {\"image\": \"https://gist.githubusercontent.com/bfirsh/3c2115692682ae260932a67d93fd94a8/raw/56b19f53f7643bb6c0b822c410c366c3a6244de2/mystery.jpg\"}}'\n</code></pre> <p>As a shorthand, you can add the Docker image's name as an extra line in <code>cog.yaml</code>:</p> <pre><code>image: \"r8.im/replicate/resnet\"\n</code></pre> <p>Once you've done this, you can use <code>cog push</code> to build and push the image to a Docker registry:</p> <pre><code>cog push\n# Building r8.im/replicate/resnet...\n# Pushing r8.im/replicate/resnet...\n# Pushed!\n</code></pre> <p>The Docker image is now accessible to anyone or any system that has access to this Docker registry.</p> <p>Note Model repos often contain large data files, like weights and checkpoints. If you put these files in their own subdirectory and run <code>cog build</code> with the <code>--separate-weights</code> flag, Cog will copy these files into a separate Docker layer, which reduces the time needed to rebuild after making changes to code.</p> <pre><code># \u2705 Yes\n.\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 weights.ckpt\n\u251c\u2500\u2500 predict.py\n\u2514\u2500\u2500 cog.yaml\n\n# \u274c No\n.\n\u251c\u2500\u2500 weights.ckpt # &lt;- Don't put weights in root directory\n\u251c\u2500\u2500 predict.py\n\u2514\u2500\u2500 cog.yaml\n\n# \u274c No\n.\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u251c\u2500\u2500 weights.ckpt\n\u2502   \u2514\u2500\u2500 load_weights.py # &lt;- Don't put code in weights directory\n\u251c\u2500\u2500 predict.py\n\u2514\u2500\u2500 cog.yaml\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<p>Those are the basics! Next, you might want to take a look at:</p> <ul> <li>A guide to help you set up your own model on Cog.</li> <li>A guide explaining how to deploy a model.</li> <li>Reference for <code>cog.yaml</code></li> <li>Reference for the Python library</li> </ul>"},{"location":"http/","title":"HTTP API","text":"<p>When a Cog Docker image is run, it serves an HTTP API for making predictions. For more information, take a look at the documentation for deploying models.</p>"},{"location":"http/#contents","title":"Contents","text":"<ul> <li>Running the server</li> <li>Stopping the server</li> <li>API</li> <li><code>GET /openapi.json</code></li> <li><code>POST /predictions</code> (synchronous)</li> <li><code>POST /predictions</code> (asynchronous)<ul> <li>Webhooks</li> </ul> </li> <li><code>PUT /predictions/</code> (synchronous)</li> <li><code>PUT /predictions/</code> (asynchronous)</li> <li><code>POST /predictions/{prediction_id}/cancel</code></li> </ul>"},{"location":"http/#running-the-server","title":"Running the server","text":"<p>First, build your model:</p> <pre><code>cog build -t my-model\n</code></pre> <p>Then, start the Docker container:</p> <pre><code># If your model uses a CPU:\ndocker run -d -p 5001:5000 my-model\n\n# If your model uses a GPU:\ndocker run -d -p 5001:5000 --gpus all my-model\n\n# If you're on an M1 Mac:\ndocker run -d -p 5001:5000 --platform=linux/amd64 my-model\n</code></pre> <p>The server is now running locally on port 5001.</p> <p>To view the OpenAPI schema, open localhost:5001/openapi.json in your browser or use cURL to make requests:</p> <pre><code>curl http://localhost:5001/openapi.json\n</code></pre>"},{"location":"http/#stopping-the-server","title":"Stopping the server","text":"<p>To stop the server, run:</p> <pre><code>docker kill my-model\n</code></pre>"},{"location":"http/#api","title":"API","text":""},{"location":"http/#get-openapijson","title":"<code>GET /openapi.json</code>","text":"<p>The OpenAPI specification of the API, which is derived from the input and output types specified in your model's Predictor and Training objects.</p>"},{"location":"http/#post-predictions-synchronous","title":"<code>POST /predictions</code> (synchronous)","text":"<p>Make a single prediction. The request body should be a JSON object with the following fields:</p> <ul> <li><code>input</code>: a JSON object with the same keys as the arguments to the <code>predict()</code> function. Any <code>File</code> or <code>Path</code> inputs are passed as URLs.</li> <li><code>output_file_prefix</code>: A base URL to upload output files to. </li> </ul> <p>The response is a JSON object with the following fields:</p> <ul> <li><code>status</code>: Either <code>succeeded</code> or <code>failed</code>.</li> <li><code>output</code>: The return value of the <code>predict()</code> function.</li> <li><code>error</code>: If <code>status</code> is <code>failed</code>, the error message.</li> </ul> <p>For example:</p> <pre><code>POST /predictions HTTP/1.1\nContent-Type: application/json; charset=utf-8\n\n{\n    \"input\": {\n        \"image\": \"https://example.com/image.jpg\",\n        \"text\": \"Hello world!\"\n    }\n}\n</code></pre> <p>Responds with:</p> <pre><code>HTTP/1.1 200 OK\nContent-Type: application/json\n\n{\n    \"status\": \"succeeded\",\n    \"output\": \"data:image/png;base64,...\"\n}\n</code></pre> <p>Or, with curl:</p> <pre><code>curl -X POST -H \"Content-Type: application/json\" -d '{\"input\": {\"image\": \"https://example.com/image.jpg\", \"text\": \"Hello world!\"}}' http://localhost:5000/predictions\n</code></pre>"},{"location":"http/#post-predictions-asynchronous","title":"<code>POST /predictions</code> (asynchronous)","text":"<p>Make a single prediction without waiting for the prediction to complete.</p> <p>Callers can specify an HTTP header of <code>Prefer: respond-async</code> when calling the <code>POST /predictions</code> endpoint. If provided, the request will return immediately after starting the prediction with an HTTP <code>202 Accepted</code> status and a prediction object in status <code>processing</code>.</p> <pre><code>POST /predictions HTTP/1.1\nContent-Type: application/json; charset=utf-8\nPrefer: respond-async\n\n{\n    \"input\": {\"prompt\": \"A picture of an onion with sunglasses\"}\n}\n</code></pre> <p>The only supported mechanism for receiving updates on the status of predictions started asynchronously is via webhooks. There is as yet no support for polling for prediction status.</p> <p>Note 1: that while this allows clients to create predictions \"asynchronously,\" Cog can only run one prediction at a time, and it is currently the caller's responsibility to make sure that earlier predictions are complete before new ones are created.</p> <p>Note 2: predictions created asynchronously use a different mechanism for file upload than those created using the synchronous API. You must specify an <code>--upload-url</code> when running the Cog server process. All uploads will be <code>PUT</code> using the provided <code>--upload-url</code> as a prefix, in much the same way that <code>output_file_prefix</code> worked. There is currently no single upload mechanism which works the same way for both synchronous and asynchronous prediction creation. This will be addressed in a future version of Cog.</p>"},{"location":"http/#webhooks","title":"Webhooks","text":"<p>Clients can (and should, if a prediction is created asynchronously) provide a <code>webhook</code> parameter at the top level of the prediction request, e.g.</p> <pre><code>POST /predictions HTTP/1.1\nContent-Type: application/json; charset=utf-8\nPrefer: respond-async\n\n{\n    \"input\": {\"prompt\": \"A picture of an onion with sunglasses\"},\n    \"webhook\": \"https://example.com/webhook/prediction\"\n}\n</code></pre> <p>Cog will make requests to the URL supplied with the state of the prediction object in the request body. Requests are made when specific events occur during the prediction, namely:</p> <ul> <li><code>start</code>: immediately on prediction start</li> <li><code>output</code>: each time a prediction generates an output (note that predictions can generate multiple outputs)</li> <li><code>logs</code>: each time log output is generated by a prediction</li> <li><code>completed</code>: when the prediction reaches a terminal state (succeeded/canceled/failed)</li> </ul> <p>Requests for event types <code>output</code> and <code>logs</code> will be sent at most once every 500ms. This interval is currently not configurable. Requests for event types <code>start</code> and <code>completed</code> will be sent immediately.</p> <p>By default, Cog will send requests for all event types. Clients can change which events trigger webhook requests by specifying <code>webhook_events_filter</code> in the prediction request. For example, if you only wanted requests to be sent at the start and end of the prediction, you would provide:</p> <pre><code>POST /predictions HTTP/1.1\nContent-Type: application/json; charset=utf-8\nPrefer: respond-async\n\n{\n    \"input\": {\"prompt\": \"A picture of an onion with sunglasses\"},\n    \"webhook\": \"https://example.com/webhook/prediction\",\n    \"webhook_events_filter\": [\"start\", \"completed\"]\n}\n</code></pre>"},{"location":"http/#put-predictionsprediction_id-synchronous","title":"<code>PUT /predictions/&lt;prediction_id&gt;</code> (synchronous)","text":"<p>Make a single prediction.</p> <p>This is the idempotent version of the <code>POST /predictions</code> endpoint. If you call it multiple times with the same ID (for example, because of a network interruption) and the prediction is still running, the request will not create further predictions but will wait for the original prediction to complete.</p> <p>Note: It is currently the caller's responsibility to ensure that the supplied prediction ID is unique. We recommend you use base32-encoded UUID4s (stripped of any padding characters) to ensure forward compatibility: these will be 26 ASCII characters long.</p>"},{"location":"http/#put-predictionsprediction_id-asynchronous","title":"<code>PUT /predictions/&lt;prediction_id&gt;</code> (asynchronous)","text":"<p>Make a single prediction without waiting for the prediction to complete.</p> <p>Callers can specify an HTTP header of <code>Prefer: respond-async</code> when calling the <code>PUT /predictions/&lt;prediction_id&gt;</code> endpoint. If provided, the request will return immediately after starting the prediction with an HTTP <code>202 Accepted</code> status and a prediction object in status <code>processing</code>.</p> <p>This is the idempotent version of the asynchronous <code>POST /predictions</code> endpoint. If you call it multiple times with the same ID (for example, because of a network interruption) and the prediction is still running, the request will not create further predictions. The caller will receive a 202 Accepted response with the initial state of the prediction.</p> <p>Note 1: It is currently the caller's responsibility to ensure that the supplied prediction ID is unique. We recommend you use base32-encoded UUID4s (stripped of any padding characters) to ensure forward compatibility: these will be 26 ASCII characters long.</p> <p>Note 2: As noted earlier, Cog can only run one prediction at a time, and it is the caller's responsibility to make sure that earlier predictions are complete before new ones (with new IDs) are created.</p>"},{"location":"http/#post-predictionsprediction_idcancel","title":"<code>POST /predictions/&lt;prediction_id&gt;/cancel</code>","text":"<p>While an asynchronous prediction is running, clients can cancel it by making a request to <code>POST /predictions/&lt;prediction_id&gt;/cancel</code>. The prediction <code>id</code> must have been supplied when creating the prediction. Predictions created without a supplied <code>id</code> field will not be cancelable.</p> <p>For example, if a prediction is created with</p> <pre><code>POST /predictions HTTP/1.1\nContent-Type: application/json; charset=utf-8\nPrefer: respond-async\n\n{\n    \"id\": \"abcd1234\",\n    \"input\": {\"prompt\": \"A picture of an onion with sunglasses\"},\n}\n</code></pre> <p>it can be canceled with</p> <pre><code>POST /predictions/abcd1234/cancel HTTP/1.1\n</code></pre> <p>Use of the cancelation API to cancel predictions started \"synchronously\" (i.e. without the <code>Prefer: respond-async</code> header) is currently not supported. This may change in a future release of Cog.</p>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Cog plays nicely with Jupyter notebooks.</p>"},{"location":"notebooks/#install-the-jupyterlab-python-package","title":"Install the jupyterlab Python package","text":"<p>First, add <code>jupyterlab</code> to the <code>python_packages</code> array in your <code>cog.yaml</code> file:</p> <pre><code>build:\n  python_packages:\n    - \"jupyterlab==3.3.4\"\n</code></pre>"},{"location":"notebooks/#run-a-notebook","title":"Run a notebook","text":"<p>Cog can run notebooks in the environment you've defined in <code>cog.yaml</code> with the following command:</p> <pre><code>cog run -p 8888 jupyter notebook --allow-root --ip=0.0.0.0\n</code></pre>"},{"location":"notebooks/#use-notebook-code-in-your-predictor","title":"Use notebook code in your predictor","text":"<p>You can also import a notebook into your Cog Predictor file.</p> <p>First, export your notebook to a Python file:</p> <pre><code>jupyter nbconvert --to script my_notebook.ipynb # creates my_notebook.py\n</code></pre> <p>Then import the exported Python script into your <code>predict.py</code> file. Any functions or variables defined in your notebook will be available to your predictor:</p> <pre><code>from cog import BasePredictor, Input\n\nimport my_notebook\n\nclass Predictor(BasePredictor):\n    def predict(self, prompt: str = Input(description=\"string prompt\")) -&gt; str:\n      output = my_notebook.do_stuff(prompt)\n      return output\n</code></pre>"},{"location":"private-package-registry/","title":"Private package registry","text":"<p>This guide describes how to build a Docker image with Cog that fetches Python packages from a private registry during setup.</p>"},{"location":"private-package-registry/#pipconf","title":"<code>pip.conf</code>","text":"<p>In a directory outside your Cog project, create a <code>pip.conf</code> file with an <code>index-url</code> set to the registry's URL with embedded credentials.</p> <pre><code>[global]\nindex-url = https://username:password@my-private-registry.com\n</code></pre> <p>Warning Be careful not to commit secrets in Git or include them in Docker images. If your Cog project contains any sensitive files, make sure they're listed in <code>.gitignore</code> and <code>.dockerignore</code>.</p>"},{"location":"private-package-registry/#cogyaml","title":"<code>cog.yaml</code>","text":"<p>In your project's <code>cog.yaml</code> file, add a setup command to run <code>pip install</code> with a secret configuration file mounted to <code>/etc/pip.conf</code>.</p> <pre><code>build:\n  run:\n    - command: pip install\n      mounts:\n        - type: secret\n          id: pip\n          target: /etc/pip.conf\n</code></pre>"},{"location":"private-package-registry/#build","title":"Build","text":"<p>When building or pushing your model with Cog, pass the <code>--secret</code> option with an <code>id</code> matching the one specified in <code>cog.yaml</code>, along with a path to your local <code>pip.conf</code> file.</p> <pre><code>$ cog build --secret id=pip,source=/path/to/pip.conf\n</code></pre> <p>Using a secret mount allows the private registry credentials to be securely passed to the <code>pip install</code> setup command, without baking them into the Docker image.</p> <p>Warning If you run <code>cog build</code> or <code>cog push</code> and then change the contents of a secret source file, the cached version of the file will be used on subsequent builds, ignoring any changes you made. To update the contents of the target secret file, either change the <code>id</code> value in <code>cog.yaml</code> and the <code>--secret</code> option, or pass the <code>--no-cache</code> option to bypass the cache entirely.</p>"},{"location":"python/","title":"Prediction interface reference","text":"<p>This document defines the API of the <code>cog</code> Python module, which is used to define the interface for running predictions on your model.</p> <p>Tip: Run <code>cog init</code> to generate an annotated <code>predict.py</code> file that can be used as a starting point for setting up your model.</p>"},{"location":"python/#contents","title":"Contents","text":"<ul> <li><code>BasePredictor</code></li> <li><code>Predictor.setup()</code></li> <li><code>Predictor.predict(**kwargs)</code><ul> <li>Streaming output</li> </ul> </li> <li><code>Input(**kwargs)</code></li> <li>Output</li> <li>Returning an object</li> <li>Returning a list</li> <li>Input and output types</li> <li><code>File()</code></li> <li><code>Path()</code></li> </ul>"},{"location":"python/#basepredictor","title":"<code>BasePredictor</code>","text":"<p>You define how Cog runs predictions on your model by defining a class that inherits from <code>BasePredictor</code>. It looks something like this:</p> <pre><code>from cog import BasePredictor, Path, Input\nimport torch\n\nclass Predictor(BasePredictor):\n    def setup(self):\n        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n        self.model = torch.load(\"weights.pth\")\n\n    def predict(self,\n            image: Path = Input(description=\"Image to enlarge\"),\n            scale: float = Input(description=\"Factor to scale image by\", default=1.5)\n    ) -&gt; Path:\n        \"\"\"Run a single prediction on the model\"\"\"\n        # ... pre-processing ...\n        output = self.model(image)\n        # ... post-processing ...\n        return output\n</code></pre> <p>Your Predictor class should define two methods: <code>setup()</code> and <code>predict()</code>.</p>"},{"location":"python/#predictorsetup","title":"<code>Predictor.setup()</code>","text":"<p>Prepare the model so multiple predictions run efficiently.</p> <p>Use this optional method to include any expensive one-off operations in here like loading trained models, instantiate data transformations, etc.</p> <p>Many models use this method to download their weights (e.g. using <code>pget</code>). This has some advantages:</p> <ul> <li>Smaller image sizes</li> <li>Faster build times</li> <li>Faster pushes and inference on Replicate</li> </ul> <p>However, this may also significantly increase your <code>setup()</code> time.</p> <p>As an alternative, some choose to store their weights directly in the image. You can simply leave your weights in the directory alongside your <code>cog.yaml</code> and ensure they are not excluded in your <code>.dockerignore</code> file.</p> <p>While this will increase your image size and build time, it offers other advantages:</p> <ul> <li>Faster <code>setup()</code> time</li> <li>Ensures idempotency and reduces your model's reliance on external systems</li> <li>Preserves reproducibility as your model will be self-contained in the image</li> </ul> <p>When using this method, you should use the <code>--separate-weights</code> flag on <code>cog build</code> to store weights in a separate layer.</p>"},{"location":"python/#predictorpredictkwargs","title":"<code>Predictor.predict(**kwargs)</code>","text":"<p>Run a single prediction.</p> <p>This required method is where you call the model that was loaded during <code>setup()</code>, but you may also want to add pre- and post-processing code here.</p> <p>The <code>predict()</code> method takes an arbitrary list of named arguments, where each argument name must correspond to an <code>Input()</code> annotation.</p> <p><code>predict()</code> can return strings, numbers, <code>cog.Path</code> objects representing files on disk, or lists or dicts of those types. You can also define a custom <code>Output()</code> for more complex return types.</p>"},{"location":"python/#streaming-output","title":"Streaming output","text":"<p>Cog models can stream output as the <code>predict()</code> method is running. For example, a language model can output tokens as they're being generated and an image generation model can output a images they are being generated.</p> <p>To support streaming output in your Cog model, add <code>from typing import Iterator</code> to your predict.py file. The <code>typing</code> package is a part of Python's standard library so it doesn't need to be installed. Then add a return type annotation to the <code>predict()</code> method in the form <code>-&gt; Iterator[&lt;type&gt;]</code> where <code>&lt;type&gt;</code> can be one of <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>cog.File</code>, or <code>cog.Path</code>.</p> <pre><code>from cog import BasePredictor, Path\nfrom typing import Iterator\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; Iterator[Path]:\n        done = False\n        while not done:\n            output_path, done = do_stuff()\n            yield Path(output_path)\n</code></pre> <p>If you're streaming text output, you can use <code>ConcatenateIterator</code> to hint that the output should be concatenated together into a single string. This is useful on Replicate to display the output as a string instead of a list of strings.</p> <pre><code>from cog import BasePredictor, Path, ConcatenateIterator\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; ConcatenateIterator[str]:\n        tokens = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n        for token in tokens:\n            yield token + \" \"\n</code></pre>"},{"location":"python/#inputkwargs","title":"<code>Input(**kwargs)</code>","text":"<p>Use cog's <code>Input()</code> function to define each of the parameters in your <code>predict()</code> method:</p> <pre><code>class Predictor(BasePredictor):\n    def predict(self,\n            image: Path = Input(description=\"Image to enlarge\"),\n            scale: float = Input(description=\"Factor to scale image by\", default=1.5, ge=1.0, le=10.0)\n    ) -&gt; Path:\n</code></pre> <p>The <code>Input()</code> function takes these keyword arguments:</p> <ul> <li><code>description</code>: A description of what to pass to this input for users of the model.</li> <li><code>default</code>: A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to <code>None</code>, the input is optional.</li> <li><code>ge</code>: For <code>int</code> or <code>float</code> types, the value must be greater than or equal to this number.</li> <li><code>le</code>: For <code>int</code> or <code>float</code> types, the value must be less than or equal to this number.</li> <li><code>min_length</code>: For <code>str</code> types, the minimum length of the string.</li> <li><code>max_length</code>: For <code>str</code> types, the maximum length of the string.</li> <li><code>regex</code>: For <code>str</code> types, the string must match this regular expression.</li> <li><code>choices</code>: For <code>str</code> or <code>int</code> types, a list of possible values for this input.</li> </ul> <p>Each parameter of the <code>predict()</code> method must be annotated with a type like <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, etc. See Input and output types for the full list of supported types.</p> <p>Using the <code>Input</code> function provides better documentation and validation constraints to the users of your model, but it is not strictly required. You can also specify default values for your parameters using plain Python, or omit default assignment entirely:</p> <pre><code>class Predictor(BasePredictor):\n    def predict(self,\n        prompt: str = \"default prompt\", # this is valid\n        iterations: int                 # also valid\n    ) -&gt; str:\n        # ...\n</code></pre>"},{"location":"python/#output","title":"Output","text":"<p>Cog predictors can return a simple data type like a string, number, float, or boolean. Use Python's <code>-&gt; &lt;type&gt;</code> syntax to annotate the return type.</p> <p>Here's an example of a predictor that returns a string:</p> <pre><code>from cog import BasePredictor\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; str:\n        return \"hello\"\n</code></pre>"},{"location":"python/#returning-an-object","title":"Returning an object","text":"<p>To return a complex object with multiple values, define an <code>Output</code> object with multiple fields to return from your <code>predict()</code> method:</p> <pre><code>from cog import BasePredictor, BaseModel, File\n\nclass Output(BaseModel):\n    file: File\n    text: str\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; Output:\n        return Output(text=\"hello\", file=io.StringIO(\"hello\"))\n</code></pre> <p>Each of the output object's properties must be one of the supported output types. For the full list, see Input and output types. Also, make sure to name the output class as <code>Output</code> and nothing else.</p>"},{"location":"python/#returning-a-list","title":"Returning a list","text":"<p>The <code>predict()</code> method can return a list of any of the supported output types. Here's an example that outputs multiple files:</p> <pre><code>from cog import BasePredictor, Path\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; list[Path]:\n        predictions = [\"foo\", \"bar\", \"baz\"]\n        output = []\n        for i, prediction in enumerate(predictions):\n            out_path = Path(f\"/tmp/out-{i}.txt\")\n            with out_path.open(\"w\") as f:\n                f.write(prediction)\n            output.append(out_path)\n        return output\n</code></pre> <p>Files are named in the format <code>output.&lt;index&gt;.&lt;extension&gt;</code>, e.g. <code>output.0.txt</code>, <code>output.1.txt</code>, and <code>output.2.txt</code> from the example above.</p>"},{"location":"python/#optional-properties","title":"Optional properties","text":"<p>To conditionally omit properties from the Output object, define them using <code>typing.Optional</code>:</p> <pre><code>from cog import BaseModel, BasePredictor, Path\nfrom typing import Optional\n\nclass Output(BaseModel):\n    score: Optional[float]\n    file: Optional[Path]\n\nclass Predictor(BasePredictor):\n    def predict(self) -&gt; Output:\n        if condition:\n            return Output(score=1.5)\n        else:\n            return Output(file=io.StringIO(\"hello\"))\n</code></pre>"},{"location":"python/#input-and-output-types","title":"Input and output types","text":"<p>Each parameter of the <code>predict()</code> method must be annotated with a type. The method's return type must also be annotated. The supported types are:</p> <ul> <li><code>str</code>: a string</li> <li><code>int</code>: an integer</li> <li><code>float</code>: a floating point number</li> <li><code>bool</code>: a boolean</li> <li><code>cog.File</code>: a file-like object representing a file</li> <li><code>cog.Path</code>: a path to a file on disk</li> </ul>"},{"location":"python/#file","title":"<code>File()</code>","text":"<p>The <code>cog.File</code> object is used to get files in and out of models. It represents a file handle.</p> <p>For models that return a <code>cog.File</code> object, the prediction output returned by Cog's built-in HTTP server will be a URL.</p> <pre><code>from cog import BasePredictor, File, Input, Path\nfrom PIL import Image\n\nclass Predictor(BasePredictor):\n    def predict(self, source_image: File = Input(description=\"Image to enlarge\")) -&gt; File:\n        pillow_img = Image.open(source_image)\n        upscaled_image = do_some_processing(pillow_img)\n        return File(upscaled_image)\n</code></pre>"},{"location":"python/#path","title":"<code>Path()</code>","text":"<p>The <code>cog.Path</code> object is used to get files in and out of models. It represents a path to a file on disk.</p> <p><code>cog.Path</code> is a subclass of Python's <code>pathlib.Path</code> and can be used as a drop-in replacement.</p> <p>For models that return a <code>cog.Path</code> object, the prediction output returned by Cog's built-in HTTP server will be a URL.</p> <p>This example takes an input file, resizes it, and returns the resized image:</p> <pre><code>import tempfile\nfrom cog import BasePredictor, Input, Path\n\nclass Predictor(BasePredictor):\n    def predict(self, image: Path = Input(description=\"Image to enlarge\")) -&gt; Path:\n        upscaled_image = do_some_processing(image)\n\n        # To output `cog.Path` objects the file needs to exist, so create a temporary file first.\n        # This file will automatically be deleted by Cog after it has been returned.\n        output_path = Path(tempfile.mkdtemp()) / \"upscaled.png\"\n        upscaled_image.save(output_path)\n        return Path(output_path)\n</code></pre>"},{"location":"python/#list","title":"<code>List</code>","text":"<p>The List type is also supported in inputs. It can hold any supported type.</p> <p>Example for List[Path]: <pre><code>class Predictor(BasePredictor):\n   def predict(self, paths: list[Path]) -&gt; str:\n       output_parts = []  # Use a list to collect file contents\n       for path in paths:\n           with open(path) as f:\n             output_parts.append(f.read())\n       return \"\".join(output_parts)\n</code></pre> The corresponding cog command: <pre><code>$ echo test1 &gt; 1.txt\n$ echo test2 &gt; 2.txt\n$ cog predict -i paths=@1.txt -i paths=@2.txt\nRunning prediction...\ntest1\n\ntest2\n</code></pre> - Note the repeated inputs with the same name \"paths\" which constitute the list</p>"},{"location":"redis/","title":"Redis queue API","text":"<p>Note: The redis queue API is no longer supported and has been removed from Cog.</p>"},{"location":"training/","title":"Training interface reference","text":"<p>[!NOTE] The training API is still experimental, and is subject to change.</p> <p>Cog's training API allows you to define a fine-tuning interface for an existing Cog model, so users of the model can bring their own training data to create derivative fune-tuned models. Real-world examples of this API in use include fine-tuning SDXL with images or fine-tuning Llama 2 with structured text.</p>"},{"location":"training/#how-it-works","title":"How it works","text":"<p>If you've used Cog before, you've probably seen the Predictor class, which defines the interface for creating predictions against your model. Cog's training API works similarly: You define a Python function that describes the inputs and outputs of the training process. The inputs are things like training data, epochs, batch size, seed, etc. The output is typically a file with the fine-tuned weights.</p> <p><code>cog.yaml</code>:</p> <pre><code>build:\n  python_version: \"3.10\"\ntrain: \"train.py:train\"\n</code></pre> <p><code>train.py</code>:</p> <pre><code>from cog import BasePredictor, File\nimport io\n\ndef train(param: str) -&gt; File:\n    return io.StringIO(\"hello \" + param)\n</code></pre> <p>Then you can run it like this:</p> <pre><code>$ cog train -i param=train\n...\n\n$ cat weights\nhello train\n</code></pre>"},{"location":"training/#inputkwargs","title":"<code>Input(**kwargs)</code>","text":"<p>Use Cog's <code>Input()</code> function to define each of the parameters in your <code>train()</code> function:</p> <pre><code>from cog import Input, Path\n\ndef train(\n    train_data: Path = Input(description=\"HTTPS URL of a file containg training data\"),\n    learning_rate: float = Input(description=\"learning rate, for learning!\", default=1e-4, ge=0),\n    seed: int = Input(description=\"random seed to use for training\", default=None)\n) -&gt; str:\n  return \"hello, weights\"\n</code></pre> <p>The <code>Input()</code> function takes these keyword arguments:</p> <ul> <li><code>description</code>: A description of what to pass to this input for users of the model.</li> <li><code>default</code>: A default value to set the input to. If this argument is not passed, the input is required. If it is explicitly set to <code>None</code>, the input is optional.</li> <li><code>ge</code>: For <code>int</code> or <code>float</code> types, the value must be greater than or equal to this number.</li> <li><code>le</code>: For <code>int</code> or <code>float</code> types, the value must be less than or equal to this number.</li> <li><code>min_length</code>: For <code>str</code> types, the minimum length of the string.</li> <li><code>max_length</code>: For <code>str</code> types, the maximum length of the string.</li> <li><code>regex</code>: For <code>str</code> types, the string must match this regular expression.</li> <li><code>choices</code>: For <code>str</code> or <code>int</code> types, a list of possible values for this input.</li> </ul> <p>Each parameter of the <code>train()</code> function must be annotated with a type like <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, etc. See Input and output types for the full list of supported types.</p> <p>Using the <code>Input</code> function provides better documentation and validation constraints to the users of your model, but it is not strictly required. You can also specify default values for your parameters using plain Python, or omit default assignment entirely:</p> <pre><code>def predict(self,\n  training_data: str = \"foo bar\", # this is valid\n  iterations: int                 # also valid\n) -&gt; str:\n  # ...\n</code></pre>"},{"location":"training/#training-output","title":"Training Output","text":"<p>Training output is typically a binary weights file. To return a custom output object or a complex object with multiple values, define a <code>TrainingOutput</code> object with multiple fields to return from your <code>train()</code> function, and specify it as the return type for the train function using Python's <code>-&gt;</code> return type annotation:</p> <pre><code>from cog import BaseModel, Input, Path\n\nclass TrainingOutput(BaseModel):\n    weights: Path\n\ndef train(\n    train_data: Path = Input(description=\"HTTPS URL of a file containg training data\"),\n    learning_rate: float = Input(description=\"learning rate, for learning!\", default=1e-4, ge=0),\n    seed: int = Input(description=\"random seed to use for training\", default=42)\n) -&gt; TrainingOutput:\n  weights_file = generate_weights(\"...\")\n  return TrainingOutput(weights=Path(weights_file))\n</code></pre>"},{"location":"training/#testing","title":"Testing","text":"<p>If you are doing development of a Cog model like Llama or SDXL, you can test that the fine-tuned code path works before pushing by specifying a <code>COG_WEIGHTS</code> environment variable when running <code>predict</code>:</p> <pre><code>cog predict -e COG_WEIGHTS=https://replicate.delivery/pbxt/xyz/weights.tar -i prompt=\"a photo of TOK\"\n</code></pre>"},{"location":"yaml/","title":"<code>cog.yaml</code> reference","text":"<p><code>cog.yaml</code> defines how to build a Docker image and how to run predictions on your model inside that image.</p> <p>It has three keys: <code>build</code>, <code>image</code>, and <code>predict</code>. It looks a bit like this:</p> <pre><code>build:\n  python_version: \"3.11\"\n  python_packages:\n    - pytorch==2.0.1\n  system_packages:\n    - \"ffmpeg\"\n    - \"git\"\npredict: \"predict.py:Predictor\"\n</code></pre> <p>Tip: Run <code>cog init</code> to generate an annotated <code>cog.yaml</code> file that can be used as a starting point for setting up your model.</p>"},{"location":"yaml/#build","title":"<code>build</code>","text":"<p>This stanza describes how to build the Docker image your model runs in. It contains various options within it:</p>"},{"location":"yaml/#cuda","title":"<code>cuda</code>","text":"<p>Cog automatically picks the correct version of CUDA to install, but this lets you override it for whatever reason.</p> <p>For example:</p> <pre><code>build:\n  cuda: \"11.1\"\n</code></pre>"},{"location":"yaml/#gpu","title":"<code>gpu</code>","text":"<p>Enable GPUs for this model. When enabled, the nvidia-docker base image will be used, and Cog will automatically figure out what versions of CUDA and cuDNN to use based on the version of Python, PyTorch, and Tensorflow that you are using.</p> <p>For example:</p> <pre><code>build:\n  gpu: true\n</code></pre> <p>When you use <code>cog run</code> or <code>cog predict</code>, Cog will automatically pass the <code>--gpus=all</code> flag to Docker. When you run a Docker image built with Cog, you'll need to pass this option to <code>docker run</code>.</p>"},{"location":"yaml/#python_packages","title":"<code>python_packages</code>","text":"<p>A list of Python packages to install from the PyPi package index, in the format <code>package==version</code>. For example:</p> <pre><code>build:\n  python_packages:\n    - pillow==8.3.1\n    - tensorflow==2.5.0\n</code></pre> <p>To install Git-hosted Python packages, add <code>git</code> to the <code>system_packages</code> list, then use the <code>git+https://</code> syntax to specify the package name. For example:</p> <pre><code>build:\n  system_packages:\n    - \"git\"\n  python_packages:\n    - \"git+https://github.com/m-bain/whisperX.git\"\n</code></pre>"},{"location":"yaml/#python_requirements","title":"<code>python_requirements</code>","text":"<p>A pip requirements file specifying the Python packages to install. For example:</p> <pre><code>build:\n  python_requirements: requirements.txt\n</code></pre> <p>Your <code>cog.yaml</code> file can set either <code>python_packages</code> or <code>python_requirements</code>, but not both. Use <code>python_requirements</code> when you need to configure options like <code>--extra-index-url</code> or <code>--trusted-host</code> to fetch Python package dependencies.</p>"},{"location":"yaml/#python_version","title":"<code>python_version</code>","text":"<p>The minor (<code>3.11</code>) or patch (<code>3.11.1</code>) version of Python to use. For example:</p> <pre><code>build:\n  python_version: \"3.11.1\"\n</code></pre> <p>Cog supports all active branches of Python: 3.8, 3.9, 3.10, 3.11, 3.12.</p> <p>Note that these are the versions supported in the Docker container, not your host machine. You can run any version(s) of Python you wish on your host machine.</p>"},{"location":"yaml/#run","title":"<code>run</code>","text":"<p>A list of setup commands to run in the environment\u00a0after your system packages and Python packages have been installed. If you're familiar with Docker, it's like a <code>RUN</code> instruction in your <code>Dockerfile</code>.</p> <p>For example:</p> <pre><code>build:\n  run:\n    - curl -L https://github.com/cowsay-org/cowsay/archive/refs/tags/v3.7.0.tar.gz | tar -xzf -\n    - cd cowsay-3.7.0 &amp;&amp; make install\n</code></pre> <p>Your code is not available to commands in <code>run</code>. This is so we can build your image efficiently when running locally.</p> <p>Each command in <code>run</code> can be either a string or a dictionary in the following format:</p> <pre><code>build:\n  run:\n    - command: pip install\n      mounts:\n        - type: secret\n          id: pip\n          target: /etc/pip.conf\n</code></pre> <p>You can use secret mounts to securely pass credentials to setup commands, without baking them into the image. For more information, see Dockerfile reference.</p>"},{"location":"yaml/#system_packages","title":"<code>system_packages</code>","text":"<p>A list of Ubuntu APT packages to install. For example:</p> <pre><code>build:\n  system_packages:\n    - \"ffmpeg\"\n    - \"libavcodec-dev\"\n</code></pre>"},{"location":"yaml/#image","title":"<code>image</code>","text":"<p>The name given to built Docker images. If you want to push to a registry, this should also include the registry name.</p> <p>For example:</p> <pre><code>image: \"r8.im/your-username/your-model\"\n</code></pre> <p>r8.im is Replicate's registry, but this can be any Docker registry.</p> <p>If you don't provide this, a name will be generated from the directory name.</p>"},{"location":"yaml/#predict","title":"<code>predict</code>","text":"<p>The pointer to the <code>Predictor</code> object in your code, which defines how predictions are run on your model.</p> <p>For example:</p> <pre><code>predict: \"predict.py:Predictor\"\n</code></pre> <p>See the Python API documentation for more information.</p>"},{"location":"wsl2/wsl2/","title":"Using <code>cog</code> on Windows 11 with WSL 2","text":"<ul> <li>0. Prerequisites</li> <li>1. Install the GPU driver</li> <li>2. Unlocking features</li> <li>2.1. Unlock WSL2</li> <li>2.2. Unlock virtualization</li> <li>2.3. Reboot</li> <li>3. Update MS Linux kernel</li> <li>4. Configure WSL 2</li> <li>5. Configure CUDA WSL-Ubuntu Toolkit</li> <li>6. Install Docker</li> <li>7. Install <code>cog</code> and pull an image</li> <li>8. Run a model in WSL 2</li> <li>9. References</li> </ul> <p>Running cog on Windows is now possible thanks to WSL 2. Follow this guide to enable WSL 2 and GPU passthrough on Windows 11.</p> <p>Windows 10 is not officially supported, as you need to be on an insider build in order to use GPU passthrough.</p>"},{"location":"wsl2/wsl2/#0-prerequisites","title":"0. Prerequisites","text":"<p>Before beginning installation, make sure you have:</p> <ul> <li>Windows 11.</li> <li>NVIDIA GPU.</li> <li>RTX 2000/3000 series</li> <li>Kesler/Tesla/Volta/Ampere series</li> <li>Other configurations are not guaranteed to work.</li> </ul>"},{"location":"wsl2/wsl2/#1-install-the-gpu-driver","title":"1. Install the GPU driver","text":"<p>Per NVIDIA, the first order of business is to install the latest Game Ready drivers for you NVIDIA GPU.</p> <p>https://www.nvidia.com/download/index.aspx</p> <p>I have an NVIDIA RTX 2070 Super, so filled out the form as such:</p> <p></p> <p>Click \"search\", and follow the dialogue to download and install the driver.</p> <p>Restart your computer once the driver has finished installation.</p>"},{"location":"wsl2/wsl2/#2-unlocking-features","title":"2. Unlocking features","text":"<p>Open Windows Terminal as an administrator.</p> <ul> <li>Use start to search for \"Terminal\"</li> <li>Right click -&gt; Run as administrator...</li> </ul> <p>Run the following powershell command to enable the Windows Subsystem for Linux and Virtual Machine Platform capabilities.</p>"},{"location":"wsl2/wsl2/#21-unlock-wsl2","title":"2.1. Unlock WSL2","text":"<pre><code>dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\n</code></pre> <p>If you see an error about permissions, make sure the terminal you are using is run as an administrator and that you have an account with administrator-level privileges.</p>"},{"location":"wsl2/wsl2/#22-unlock-virtualization","title":"2.2. Unlock virtualization","text":"<pre><code>dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart\n</code></pre> <p>If this command fails, make sure to enable virtualization capabilities in your computer's BIOS/UEFI. A successful output will print <code>The operation completed successfully.</code></p> <p></p>"},{"location":"wsl2/wsl2/#23-reboot","title":"2.3. Reboot","text":"<p>Before moving forward, make sure you reboot your computer so that Windows 11 will have WSL2 and virtualization available to it.</p>"},{"location":"wsl2/wsl2/#3-update-ms-linux-kernel","title":"3. Update MS Linux kernel","text":"<p>Download and run the WSL2 Linux kernel update package for x64 machines msi installer. When prompted for elevated permissions, click 'yes' to approve the installation.</p> <p>To ensure you are using the correct WSL kernel, <code>open Windows Terminal as an adminstrator</code> and enter:</p> <pre><code>wsl cat /proc/version\n</code></pre> <p>This will return a complicated string such as:</p> <pre><code>Linux version 5.10.102.1-microsoft-standard-WSL2 (oe-user@oe-host) (x86_64-msft-linux-gcc (GCC) 9.3.0, GNU ld (GNU Binutils) 2.34.0.20200220)\n</code></pre> <p>The version we are interested in is <code>Linux version 5.10.102.1</code>. At this point, you should have updated your kernel to be at least <code>Linux version 5.10.43.3</code>.</p> <p>If you can't get the correct kernel version to show:</p> <p>Open <code>Settings</code> \u2192 <code>Windows Update</code> \u2192 <code>Advanced options</code> and ensure <code>Receive updates for other Microsoft products</code> is enabled. Then go to <code>Windows Update</code> again and click <code>Check for updates</code>.</p>"},{"location":"wsl2/wsl2/#4-configure-wsl-2","title":"4. Configure WSL 2","text":"<p>First, configure Windows to use the virtualization-based version of WSL (version 2) by default. In a Windows Terminal with adminstrator priveleges, type the following:</p> <pre><code>wsl --set-default-version 2\n</code></pre> <p>Now, you will need to go to the Microsoft Store and Download Ubuntu 18.04</p> <p></p> <p>Launch the \"Ubuntu\" app available in your Start Menu. Linux will require its own user account and password, which you will need to enter now:</p> <p></p>"},{"location":"wsl2/wsl2/#5-configure-cuda-wsl-ubuntu-toolkit","title":"5. Configure CUDA WSL-Ubuntu Toolkit","text":"<p>By default, a shimmed version of the CUDA tooling is provided by your Windows GPU drivers.</p> <p>Important: you should never use instructions for installing CUDA-toolkit in a generic linux fashion. in WSL 2, you always want to use the provided <code>CUDA Toolkit using WSL-Ubuntu Package</code>.</p> <p>First, open PowerShell or Windows Command Prompt in administrator mode by right-clicking and selecting \"Run as administrator\". Then enter the following command:</p> <pre><code>wsl.exe\n</code></pre> <p>This should drop you into your running linux VM. Now you can run the following bash commands to install the correct version of cuda-toolkit for WSL-Ubuntu. Note that the version of CUDA used below may not be the version of CUDA your GPU supports.</p> <pre><code>sudo apt-key del 7fa2af80 # if this line fails, you may remove it.\nwget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\nsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/11.7.0/local_installers/cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb\nsudo dpkg -i cuda-repo-wsl-ubuntu-11-7-local_11.7.0-1_amd64.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit-11-7\n</code></pre>"},{"location":"wsl2/wsl2/#6-install-docker","title":"6. Install Docker","text":"<p>Download and install Docker Desktop for Windows. It has WSL 2 support built in by default.</p> <p>Once installed, run <code>Docker Desktop</code>, you can ignore the first-run tutorial. Go to Settings \u2192 General and ensure Use the WSL 2 based engine has a checkmark next to it. Click Apply &amp; Restart.</p> <p></p> <p>Reboot your computer one more time.</p>"},{"location":"wsl2/wsl2/#7-install-cog-and-pull-an-image","title":"7. Install <code>cog</code> and pull an image","text":"<p>Open Windows Terminal and enter your WSL 2 VM:</p> <pre><code>wsl.exe\n</code></pre> <p>Download and install <code>cog</code> inside the VM:</p> <pre><code>sudo curl -o /usr/local/bin/cog -L https://github.com/replicate/cog/releases/latest/download/cog_`uname -s`_`uname -m`\nsudo chmod +x /usr/local/bin/cog\n</code></pre> <p>Make sure it's available by typing:</p> <pre><code>which cog # should output /usr/local/bin/cog\ncog --version # should output the cog version number.\n</code></pre>"},{"location":"wsl2/wsl2/#8-run-a-model-in-wsl-2","title":"8. Run a model in WSL 2","text":"<p>Finally, make sure it works. Let's try running <code>afiaka87/glid-3-xl</code> locally:</p> <pre><code>cog predict 'r8.im/afiaka87/glid-3-xl' -i prompt=\"a fresh avocado floating in the water\" -o prediction.json\n</code></pre> <p></p> <p>While your prediction is running, you can use <code>Task Manager</code> to keep an eye on GPU memory consumption:</p> <p></p> <p>This model just barely manages to fit under 8 GB of VRAM.</p> <p>Notice that output is returned as JSON for this model as it has a complex return type. You will want to convert the base64 string in the json array to an image.</p> <p><code>jq</code> can help with this:</p> <pre><code>sudo apt install jq\n</code></pre> <p>The following bash uses <code>jq</code> to grab the first element in our prediction array and converts it from a base64 string to a <code>png</code> file.</p> <pre><code>jq -cs '.[0][0][0]' prediction.json | cut --delimiter \",\" --field 2 | base64 --ignore-garbage --decode &gt; prediction.png\n</code></pre> <p>When using WSL 2, you can access Windows binaries with the <code>.exe</code> extension. This lets you open photos easily within linux.</p> <pre><code>explorer.exe prediction.png\n</code></pre> <p></p>"},{"location":"wsl2/wsl2/#9-references","title":"9. References","text":"<ul> <li>https://docs.nvidia.com/cuda/wsl-user-guide/index.html</li> <li>https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=WSL-Ubuntu&amp;target_version=2.0</li> <li>https://www.docker.com/blog/wsl-2-gpu-support-for-docker-desktop-on-nvidia-gpus/</li> <li>https://docs.microsoft.com/en-us/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package</li> <li>https://github.com/replicate/cog</li> </ul>"}]}